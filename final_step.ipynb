{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS536 Image-to-Image Translation Final Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as tforms\n",
    "from torchvision import utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "ngpu = 1  # code cannot support multi-gpu at this time\n",
    "device = torch.device('cuda:0' if (torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Manipulation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(model: nn.Module, device, ngpu):\n",
    "    \"\"\"Transfers the models onto the specified device.\n",
    "        \n",
    "    Params:\n",
    "        model -- which model to transfer\n",
    "        device -- which device to use\n",
    "        ngpus -- how many gpus to use, if using cuda device\n",
    "    Returns:\n",
    "        The transferred model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    if device.type == 'cuda' and ngpu > 1:\n",
    "        model = nn.DataParallel(model, list(range(ngpu)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(model: nn.Module, dir_name, name):\n",
    "    \"\"\"Saves the current state dictionary to a files in given path.\n",
    "    \n",
    "    Params:\n",
    "        model -- the model to save\n",
    "        dir_name -- directory in which to store the saved files\n",
    "        name -- name that is prefixed on the files\n",
    "    \"\"\"\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), path.join(dir_name, f'{name}.pth'))\n",
    "    else:\n",
    "        torch.save(model.state_dict(), path.join(dir_name, f'{name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(model: nn.Module, dir_name, name):\n",
    "    \"\"\"Loads a state dictionary from the files in given path.\n",
    "    \n",
    "    Params:\n",
    "        model -- the model to load\n",
    "        dir_name -- directory from which to load the state\n",
    "        name -- name that is prefixed on the files\n",
    "    \"\"\"\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(torch.load(path.join(dir_name, f'{name}.pth')))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(path.join(dir_name, f'{name}.pth')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Definition\n",
    "\n",
    "The following define the basic building blocks of our architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_norm_relu(n_in, n_out, kernel_size, stride, padding=0, padding_mode='reflect', transpose=False, **kwargs):\n",
    "    \"\"\"Standard convolution -> instance norm -> relu block.\n",
    "\n",
    "    Params:\n",
    "        n_in -- number of input channels\n",
    "        n_out -- number of filters/output channels\n",
    "        kernel_size -- passed to Conv2d\n",
    "        stride -- passed to Conv2d\n",
    "        padding -- passed to Conv2d\n",
    "        padding_mode -- passed to Conv2d\n",
    "        transpose -- whether to use a regular or transposed convolution layer\n",
    "        kwargs -- other args passed to Conv2d\n",
    "    Returns:\n",
    "        A list containing a convolution, instance norm, and ReLU activation\n",
    "    \"\"\"\n",
    "    if transpose:\n",
    "        conv = nn.ConvTranspose2d(n_in, n_out, kernel_size, stride, padding, padding_mode=padding_mode, bias=True, **kwargs)\n",
    "    else:\n",
    "        conv = nn.Conv2d(n_in, n_out, kernel_size, stride, padding, padding_mode=padding_mode, bias=True, **kwargs)\n",
    "    return [conv, nn.InstanceNorm2d(n_out), nn.ReLU(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_norm_leakyrelu(n_in, n_out, slope=0.2, **kwargs):\n",
    "    \"\"\"Standard convolution -> instance norm -> leaky relu block.\n",
    "    \n",
    "    Params:\n",
    "        n_in -- number of input channels\n",
    "        n_out -- number of filters/output channels\n",
    "        slope -- slope of the leaky ReLU layer\n",
    "        kwargs -- other args passed to the convolution layer\n",
    "    Returns:\n",
    "        A list containing a convolution, instance norm, and LeakyReLU activation\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(n_in, n_out, **kwargs)\n",
    "    return [conv, nn.InstanceNorm2d(n_out), nn.LeakyReLU(slope, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Defines a residual block with 2 3x3 conv-norm-relu layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, p=None):\n",
    "        \"\"\"Initialize a residual block.\n",
    "        \n",
    "        Params:\n",
    "            k -- number of input and output channels\n",
    "            p -- dropout rate (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        model = conv_norm_relu(k, k, 3, 1, 1)\n",
    "        model.append(nn.Conv2d(k, k, 3, 1, 1, padding_mode='reflect', bias=True))\n",
    "        model.append(nn.InstanceNorm2d(k))\n",
    "        if p is not None:\n",
    "            model.append(nn.Dropout(p, inplace=True))\n",
    "        self.block = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        residual = self.block(input)\n",
    "        residual += input  # apply skip-connection\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators\n",
    "\n",
    "The generators are VAEs.\n",
    "An input image is encoded into a MVN distribution on the latent space with identity covariance,\n",
    "whereas the decoder is deterministic (unless dropout is enabled).\n",
    "\n",
    "To perform translation, different encoders are paired with different decoders;\n",
    "this assumes a shared latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional-Resnet style encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_head, n_res, in_channel, n_filter):\n",
    "        \"\"\"Initialize an encoder.\n",
    "        \n",
    "        Params:\n",
    "            n_head -- number of downsampling convolution blocks at the head\n",
    "            n_res -- number of residual blocks in the middle\n",
    "            in_channel -- number of channels in the input\n",
    "            n_filter -- number of filters to start with; doubles for each block in the head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # initial convolution\n",
    "        front = conv_norm_relu(in_channel, n_filter, 7, 1, 3)\n",
    "        # downsampling convolution blocks\n",
    "        for _ in range(n_head):\n",
    "            front += conv_norm_relu(n_filter, 2 * n_filter, 4, 2, 1)\n",
    "            n_filter *= 2\n",
    "        # middle residual blocks\n",
    "        front += [ResidualBlock(n_filter) for _ in range(n_res)]\n",
    "        self.model = nn.Sequential(*front)\n",
    "        self.out_channel = n_filter  # record the number of filters before the adjustment\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentAE(nn.Module):\n",
    "    \"\"\"Shared latent space VAE. Contains both encoder and decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_res, in_channel, p=None):\n",
    "        \"\"\"Initialize a VAE.\n",
    "\n",
    "        Params:\n",
    "            n_res -- number of residual blocks for both the encoder and decoder\n",
    "            n_channels -- number of channels in the input\n",
    "            p -- dropout probability used (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(*[ResidualBlock(in_channel) for _ in range(n_res)])\n",
    "        self.dec = nn.Sequential(*[ResidualBlock(in_channel, p) for _ in range(n_res)])\n",
    "        self.is_dist = False\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent_mean = self.enc(input)\n",
    "        sample = latent_mean + torch.randn(latent_mean.size(), device=latent_mean.device)\n",
    "        return latent_mean, self.dec(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Convolutional-Resnet style decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_tail, n_res, in_channel, out_channel, p=None):\n",
    "        \"\"\"Initialize a decoder.\n",
    "\n",
    "        Params:\n",
    "            n_tail -- number of upsampling convolution blocks at the tail\n",
    "            n_res -- number of residual blocks in the middle\n",
    "            in_channel -- number of channels in the input\n",
    "            out_channel -- desired number of channels in the output\n",
    "            p -- dropout probability used (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # residual blocks in the middle\n",
    "        model = [ResidualBlock(in_channel, p) for _ in range(n_res)]\n",
    "        # upsampling transposed convolution blocks\n",
    "        for _ in range(n_tail):\n",
    "            model += conv_norm_relu(in_channel, in_channel // 2, 4, 2, 1, padding_mode='zeros', transpose=True)\n",
    "            in_channel //= 2\n",
    "        # final convolution (use tanh)\n",
    "        model += [nn.Conv2d(in_channel, out_channel, 7, 1, 3, padding_mode='reflect', bias=True), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    \"\"\"Wraps the models necessary to perform translation between two domains.\"\"\"\n",
    "\n",
    "    def __init__(self, d1: str, d2: str, n_channel, n_conv, n_res, n_shared, n_filter, p=None):\n",
    "        \"\"\"Initializes two VAEs with shared inner weights.\n",
    "\n",
    "        Params:\n",
    "            d1 -- name of first domain\n",
    "            d2 -- name of second domain\n",
    "            n_channel -- number of input channels of an image\n",
    "            n_conv -- number of outermost conv/conv-tranpose blocks in the VAE\n",
    "            n_res -- number of residual blocks in the middle layers of the VAE\n",
    "            n_shared -- number of residual blocks that are shared\n",
    "            n_filter -- number of filters to start with in the encoder\n",
    "            p -- dropout probability in the decoders (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d1_encoder = Encoder(n_conv, n_res, n_channel, n_filter)\n",
    "        d2_encoder = Encoder(n_conv, n_res, n_channel, n_filter)\n",
    "        d1_decoder = Decoder(n_conv, n_res, d1_encoder.out_channel, n_channel, p)\n",
    "        d2_decoder = Decoder(n_conv, n_res, d2_encoder.out_channel, n_channel, p)\n",
    "        self.encoders = nn.ModuleDict({d1: d1_encoder, d2: d2_encoder})\n",
    "        self.decoders = nn.ModuleDict({d1: d1_decoder, d2: d2_decoder})\n",
    "        self.shared = LatentAE(n_shared, d1_encoder.out_channel, p)\n",
    "\n",
    "    def translate(self, input, source: str, target: str, keep_mean=True, requires_grad=True):\n",
    "        \"\"\"Translates a batch of images from the source domain to the target domain.\n",
    "\n",
    "        Params:\n",
    "            input -- input image (batch)\n",
    "            source -- source domain\n",
    "            target -- target domain (of translation)\n",
    "            keep_mean -- whether to also return the latent space mean\n",
    "            requires_grad -- whether to track the computation graph\n",
    "        Returns:\n",
    "            The translated image\n",
    "        \"\"\"\n",
    "        if requires_grad:\n",
    "            l_mean, encoding = self.shared(self.encoders[source](input))\n",
    "            output = self.decoders[target](encoding)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                l_mean, encoding = self.shared(self.encoders[source](input))\n",
    "                output = self.decoders[target](encoding)\n",
    "        if keep_mean:\n",
    "            return l_mean, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminators\n",
    "\n",
    "The discriminators are multi-scale patchGAN discriminators, as used in ACLGAN, MUNIT etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patchdisc(n_layer, in_channel, n_filter):\n",
    "    \"\"\"Makes a basic patchGAN discriminator.\n",
    "\n",
    "    Params:\n",
    "        n_layer -- number of convolution-block layers\n",
    "        in_channel -- number of channels in\n",
    "        n_filter -- number of filters to start with (doubles at each layer)\n",
    "    \"\"\"\n",
    "    # first layer has no instance norm\n",
    "    model = [nn.Conv2d(in_channel, n_filter, 4, 2, 1, padding_mode='reflect', bias=True), nn.LeakyReLU(0.2, True)]\n",
    "    for _ in range(n_layer - 1):\n",
    "        model += conv_norm_leakyrelu(n_filter, n_filter * 2, kernel_size=4, stride=2, padding=1, padding_mode='reflect', bias=True)\n",
    "        n_filter *= 2\n",
    "    model.append(nn.Conv2d(n_filter, 1, kernel_size=4, stride=1, padding=1, padding_mode='reflect'))\n",
    "    model.append(nn.Sigmoid())  # use sigmoid with MSE loss\n",
    "    return nn.Sequential(*model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSDisc(nn.Module):\n",
    "    \"\"\"Multi-scale Discriminator.\"\"\"\n",
    "\n",
    "    def __init__(self, n_scale, n_layer, in_channel, n_filter):\n",
    "        \"\"\"Initialize a discriminator.\n",
    "\n",
    "        Params:\n",
    "            n_scale -- number of scales to run discriminators on\n",
    "            n_layer -- passed to patchGAN\n",
    "            in_channel -- passed to patchGAN\n",
    "            n_filter -- passed to patchGAN\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.models = nn.ModuleList([make_patchdisc(n_layer, in_channel, n_filter) for _ in range(n_scale)])\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        outputs = []\n",
    "        for model in self.models:\n",
    "            outputs.append(model(input))\n",
    "            input = self.downsample(input)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainDiscs(nn.Module):\n",
    "    \"\"\"Wraps the discriminators associated with a particular domain.\"\"\"\n",
    "\n",
    "    def __init__(self, name: str, n_channel, n_scale, n_layer, n_filter):\n",
    "        \"\"\"Initializes two discriminators for the adv and acl losses.\n",
    "        \n",
    "        Params:\n",
    "            name -- name of the corresponding domain\n",
    "            n_channel -- number of input channels of an image\n",
    "            n_scale -- number of scales to run discriminators on\n",
    "            n_layer -- passed to patchGAN (discriminator initialization)\n",
    "            n_filter -- passed to patchGAN (discriminator initialization)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.domain = name\n",
    "        self.adv_disc = MSDisc(n_scale, n_layer, n_channel, n_filter)\n",
    "        self.acl_disc = MSDisc(n_scale, n_layer, n_channel * 2, n_filter)  # acl-disc takes two images stacked as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae regularization term\n",
    "def vae_kl(latent_mean):\n",
    "    \"\"\"Computes the KL-divergence of a latent distribution for a VAE.\n",
    "    \n",
    "    Params:\n",
    "        latent_mean -- mean of latent representation\n",
    "    Returns:\n",
    "        KL-divergence\n",
    "    \"\"\"\n",
    "    return 0.5 * torch.mean(torch.square(latent_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_recon_loss = nn.L1Loss()\n",
    "\n",
    "# generator/vae loss function\n",
    "def reconstruction_loss(input, latent_mean, output, hp):\n",
    "    \"\"\"Computes the reconstruction loss for a VAE.\n",
    "    \n",
    "    Params:\n",
    "        input -- original image\n",
    "        latent_mean -- mean of latent representation\n",
    "        output -- reconstructed image\n",
    "        hp -- dictionary of hyperparameters\n",
    "    Returns:\n",
    "        Reconstruction loss\n",
    "    \"\"\"\n",
    "    # likelihood term (L1 loss)\n",
    "    loss = hp['nll_w'] * _recon_loss(input, output)\n",
    "    # regularization term (KL-divergence)\n",
    "    loss += hp['kl_w'] * vae_kl(latent_mean)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_disc_loss = nn.MSELoss()\n",
    "\n",
    "# multi-scale discriminator evaluation and loss function\n",
    "def disc_loss(input, disc: MSDisc, label):\n",
    "    \"\"\"Evaluates the discriminator on the input and computes the BCE-loss.\n",
    "\n",
    "    Params:\n",
    "        input -- input image\n",
    "        disc -- discriminator to run the image on\n",
    "        label -- the 'ground truth' label for that image\n",
    "    Returns:\n",
    "        Discriminator loss\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    for output in disc(input):\n",
    "        truth = torch.full(output.size(), fill_value=label, device=output.device)\n",
    "        loss += _disc_loss(output, truth)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Update Code\n",
    "\n",
    "These functions merely accumulate gradients, and do not perform optimization.\n",
    "The optimizer step as well as the clearing of gradients needs to be done outside these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels used in the loss functions\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_ae_grad(input, ae: Translator, disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the gradients associated with the real input examples for the discriminators.\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        ae -- auto-encoder used\n",
    "        disc -- discriminators corresponding to the domain\n",
    "        hp -- dictionary of hyperparameters\n",
    "    Returns:\n",
    "        Computed loss values\n",
    "    \"\"\"\n",
    "    domain = disc.domain\n",
    "    output = ae.translate(input, domain, domain, keep_mean=False, requires_grad=False)\n",
    "    adv_loss = hp['adv_w'] * disc_loss(input, disc.adv_disc, real_label)  # pass real example to adv\n",
    "    adv_loss.backward()\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, output), dim=1), disc.acl_disc, real_label)  # pass reconstructed pair to acl\n",
    "    acl_loss.backward()\n",
    "    return adv_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ae_grad(input, ae: Translator, disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the gradients associated with the reconstructed images for the generators.\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        ae -- auto-encoder used\n",
    "        disc -- discriminators corresponding to the domain\n",
    "        hp -- dictionary of hyperparameters\n",
    "    Returns:\n",
    "        Computed loss values\n",
    "    \"\"\"\n",
    "    domain = disc.domain\n",
    "    l_mean, output = ae.translate(input, domain, domain)\n",
    "    r_loss = reconstruction_loss(input, l_mean, output, hp)  # vae reconstruction error\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, output), dim=1), disc.acl_disc, fake_label)  # pass reconstructed pair to acl\n",
    "    total_loss = r_loss + acl_loss\n",
    "    total_loss.backward()\n",
    "    return r_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_cycle_grad(input, gen: Translator, source_disc: DomainDiscs, target_disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the discriminator gradients for a cycle translation pass.\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        gen -- generator used for translation\n",
    "        source_disc -- discriminator for source domain\n",
    "        target_disc -- discriminator for target domain\n",
    "        hp -- dictionary of hyperparameters\n",
    "    Returns:\n",
    "        Computes loss values\n",
    "    \"\"\"\n",
    "    source = source_disc.domain\n",
    "    target = target_disc.domain\n",
    "    translated_img = gen.translate(input, source, target, keep_mean=False, requires_grad=False)\n",
    "    recovered_img = gen.translate(translated_img, target, source, keep_mean=False, requires_grad=False)\n",
    "\n",
    "    # compute loss for standard discriminators\n",
    "    adv_loss = disc_loss(translated_img, target_disc.adv_disc, fake_label)\n",
    "    adv_loss += disc_loss(recovered_img, source_disc.adv_disc, fake_label)\n",
    "    adv_loss *= hp['adv_w']\n",
    "    adv_loss.backward()\n",
    "\n",
    "    # compute loss for acl discriminator\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, recovered_img), dim=1), source_disc.acl_disc, fake_label)\n",
    "    acl_loss.backward()\n",
    "\n",
    "    return adv_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cycle_grad(input, gen: Translator, source_disc: DomainDiscs, target_disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the generator gradients for a cycle translation pass.\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        gen -- generator used for translation\n",
    "        source_disc -- discriminator for source domain\n",
    "        target_disc -- discriminator for target domain\n",
    "        hp -- dictionary of hyperparameters\n",
    "    Returns:\n",
    "        Computed loss values\n",
    "    \"\"\"\n",
    "    source = source_disc.domain\n",
    "    target = target_disc.domain\n",
    "    translated_img = gen.translate(input, source, target, keep_mean=False)\n",
    "    l_mean, recovered_img = gen.translate(translated_img, target, source)\n",
    "\n",
    "    # compute regularization losses\n",
    "    r_loss = vae_kl(l_mean)\n",
    "    r_loss *= hp['kl_w']\n",
    "\n",
    "    # compute loss for standard discriminators\n",
    "    adv_loss = disc_loss(translated_img, target_disc.adv_disc, real_label)\n",
    "    adv_loss += disc_loss(recovered_img, source_disc.adv_disc, real_label)\n",
    "    adv_loss *= hp['adv_w']\n",
    "\n",
    "    # compute loss for acl discriminator\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, recovered_img), dim=1), source_disc.acl_disc, real_label)\n",
    "    \n",
    "    total_loss = r_loss + adv_loss + acl_loss\n",
    "    total_loss.backward()\n",
    "    return adv_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_bigcycle_grad(input, s: str, b: str, t: str, s_gen: Translator, t_gen: Translator, s_disc: DomainDiscs, t_disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the discriminator gradients for a big cycle translation pass (across two domains).\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        s - source domain\n",
    "        b - bridge domain\n",
    "        t - target domain\n",
    "        s_gen -- source to bridge translator\n",
    "        t_gen -- target to bridge translator\n",
    "        s_disc -- disciminator for source domain\n",
    "        t_disc -- discriminator for target domain\n",
    "    Returns:\n",
    "        Computed loss values\n",
    "    \"\"\"\n",
    "    bridge_img = s_gen.translate(input, s, b, keep_mean=False, requires_grad=False)\n",
    "    translated_img = t_gen.translate(bridge_img, b, t, keep_mean=False, requires_grad=False)\n",
    "    rec_bridge_img = t_gen.translate(translated_img, t, b, keep_mean=False, requires_grad=False)\n",
    "    recovered_img = s_gen.translate(rec_bridge_img, b, s, keep_mean=False, requires_grad=False)\n",
    "\n",
    "    # compute loss for standard discriminators\n",
    "    adv_loss = disc_loss(translated_img, t_disc.adv_disc, fake_label)\n",
    "    adv_loss += disc_loss(recovered_img, s_disc.adv_disc, fake_label)\n",
    "    adv_loss *= hp['adv_w']\n",
    "    adv_loss.backward()\n",
    "\n",
    "    # compute loss for acl discriminator\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, recovered_img), dim=1), s_disc.acl_disc, fake_label)\n",
    "    acl_loss.backward()\n",
    "\n",
    "    return adv_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bigcycle_grad(input, s: str, b: str, t: str, s_gen: Translator, t_gen: Translator, s_disc: DomainDiscs, t_disc: DomainDiscs, hp):\n",
    "    \"\"\"Computes the generator gradients for a big cycle translation pass (across two domains).\n",
    "    \n",
    "    Params:\n",
    "        input -- input image batch\n",
    "        s - source domain\n",
    "        b - bridge domain\n",
    "        t - target domain\n",
    "        s_gen -- source to bridge translator\n",
    "        t_gen -- target to bridge translator\n",
    "        s_disc -- disciminator for source domain\n",
    "        t_disc -- discriminator for target domain\n",
    "    Returns:\n",
    "        Computed loss values\n",
    "    \"\"\"\n",
    "    bridge_img = s_gen.translate(input, s, b, keep_mean=False)\n",
    "    l_mean, translated_img = t_gen.translate(bridge_img, b, t)\n",
    "    l_mean2, rec_bridge_img = t_gen.translate(translated_img, t, b)\n",
    "    l_mean3, recovered_img = s_gen.translate(rec_bridge_img, b, s)\n",
    "\n",
    "    # compute regularization losses\n",
    "    r_loss = vae_kl(l_mean)\n",
    "    r_loss += vae_kl(l_mean2)\n",
    "    r_loss += vae_kl(l_mean3)\n",
    "    r_loss *= hp['kl_w']\n",
    "\n",
    "    # compute loss for standard discriminators\n",
    "    adv_loss = disc_loss(translated_img, t_disc.adv_disc, real_label)\n",
    "    adv_loss += disc_loss(recovered_img, s_disc.adv_disc, real_label)\n",
    "    adv_loss *= hp['adv_w']\n",
    "\n",
    "    # compute loss for acl discriminator\n",
    "    acl_loss = hp['acl_w'] * disc_loss(torch.cat((input, recovered_img), dim=1), s_disc.acl_disc, real_label)\n",
    "    \n",
    "    total_loss = r_loss + adv_loss + acl_loss\n",
    "    total_loss.backward()\n",
    "    return adv_loss.item(), acl_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader/Preprocessing\n",
    "\n",
    "The dataloader uses the basic `ImageFolder` class,\n",
    "meaning the provided directory path should contain a single subdirectory, which contains all the images desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "uncropped_img_size = 268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(path: str, batch_size=16, num_workers=2):\n",
    "    data_transform = tforms.Compose([\n",
    "        tforms.Resize(uncropped_img_size),\n",
    "        tforms.RandomCrop(img_size),\n",
    "        tforms.RandomHorizontalFlip(),\n",
    "        tforms.ToTensor(),\n",
    "        tforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(root=path, transform=data_transform)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like load_train_data but performs no preprocessing/data augmentation\n",
    "def load_test_data(path: str, batch_size=16, num_workers=2):\n",
    "    data_transform = tforms.Compose([\n",
    "        tforms.Resize(img_size),\n",
    "        tforms.CenterCrop(img_size),\n",
    "        tforms.ToTensor(),\n",
    "        tforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(root=path, transform=data_transform)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if 'Conv' in classname:\n",
    "        nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "\n",
    "def init_weights_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if 'Conv' in classname:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_translate_images(s_in, b_in, t_in, s: str, b: str, t: str, s_gen: Translator, t_gen: Translator, dir_name, name):\n",
    "    \"\"\"Translate a test batch and save the results.\n",
    "\n",
    "    Params:\n",
    "        s_in -- input batch from source domain\n",
    "        b_in -- input batch from bridge domain\n",
    "        t_in -- input batch from target domain\n",
    "        s, b, t -- domain names\n",
    "        s_gen -- translator between source and bridge\n",
    "        t_gen -- translator between bridge and target\n",
    "        vae: the shared latent network\n",
    "        dir_name: directory to save images to\n",
    "        name: image file prefix\n",
    "    \"\"\"\n",
    "    # bridge translations\n",
    "    b2s = s_gen.translate(b_in, b, s, keep_mean=False, requires_grad=False)\n",
    "    b2s2b = s_gen.translate(b2s, s, b, keep_mean=False, requires_grad=False)\n",
    "    b2t = t_gen.translate(b_in, b, t, keep_mean=False, requires_grad=False)\n",
    "    b2t2b = t_gen.translate(b2t, t, b, keep_mean=False, requires_grad=False)\n",
    "    img = vutils.make_grid(torch.cat((b_in, b2s, b2s2b, b2t, b2t2b)), nrow=b_in.size(0), normalize=True)\n",
    "    vutils.save_image(img, path.join(dir_name, f'{name}_bridge.png'))\n",
    "\n",
    "    # source translations\n",
    "    s2b = s_gen.translate(s_in, s, b, keep_mean=False, requires_grad=False)\n",
    "    s2b2t = t_gen.translate(s2b, b, t, keep_mean=False, requires_grad=False)\n",
    "    s2b2t2b = t_gen.translate(s2b2t, t, b, keep_mean=False, requires_grad=False)\n",
    "    s2b2t2b2s = s_gen.translate(s2b2t2b, b, s, keep_mean=False, requires_grad=False)\n",
    "    img = vutils.make_grid(torch.cat((s_in, s2b, s2b2t, s2b2t2b2s)), nrow=s_in.size(0), normalize=True)\n",
    "    vutils.save_image(img, path.join(dir_name, f'{name}_source.png'))\n",
    "\n",
    "    # target translations\n",
    "    t2b = t_gen.translate(t_in, t, b, keep_mean=False, requires_grad=False)\n",
    "    t2b2s = s_gen.translate(t2b, b, s, keep_mean=False, requires_grad=False)\n",
    "    t2b2s2b = s_gen.translate(t2b2s, s, b, keep_mean=False, requires_grad=False)\n",
    "    t2b2s2b2t = t_gen.translate(t2b2s2b, b, t, keep_mean=False, requires_grad=False)\n",
    "    img = vutils.make_grid(torch.cat((t_in, t2b, t2b2s, t2b2s2b2t)), nrow=t_in.size(0), normalize=True)\n",
    "    vutils.save_image(img, path.join(dir_name, f'{name}_target.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting execution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture parameters (see DomainModels constructor for details)\n",
    "n_channel = 3\n",
    "n_conv = 3\n",
    "n_res = 3\n",
    "n_shared = 1  # number of shared layers, in both encoder and decoder\n",
    "n_gen_filter = 64\n",
    "n_scale = 1  # number of scales to apply MS-disc at\n",
    "n_layer = 5  # number of layers for the discriminator\n",
    "n_disc_filter = 64\n",
    "p = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function hyperparameters\n",
    "hp = {}\n",
    "\n",
    "# for VAE-loss\n",
    "hp['nll_w'] = 100\n",
    "hp['kl_w'] = 0.1\n",
    "\n",
    "# for discriminator losses\n",
    "hp['adv_w'] = 100\n",
    "hp['acl_w'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer hyperparameters (for ADAM optimizer)\n",
    "lr = 0.0001\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "disc_update_freq = 1\n",
    "gen_update_freq = 1\n",
    "batch_size = 8\n",
    "test_batch_size = 16  # number of test images to translate and log after each epoch during training\n",
    "n_epochs = 20  # number of epochs to train for the second phase\n",
    "checkpoint_freq = 2  # save copy of models every nth epoch\n",
    "test_freq = 1  # save test translated images every nth epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up models, data, and checkpoints\n",
    "\n",
    "Domain `A` is the prerecorded synthetic pizza,\n",
    "domain `B` is the live synthetic pizza (serving as a bridge),\n",
    "and domain `C` is the real pizza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory paths\n",
    "checkpoint_dir = 'checkpoints'\n",
    "result_dir = 'training_results'\n",
    "\n",
    "A_train_dir = 'prerec_train'\n",
    "B_train_dir = 'live_train'\n",
    "C_train_dir = 'real_train'\n",
    "\n",
    "A_test_dir = 'prerec_test'\n",
    "B_test_dir = 'live_test'\n",
    "C_test_dir = 'real_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup translators between domains\n",
    "AB_gen = Translator('A', 'B', n_channel, n_conv, n_res, n_shared, n_gen_filter, p)\n",
    "BC_gen = Translator('B', 'C', n_channel, n_conv, n_res, n_shared, n_gen_filter, p)\n",
    "\n",
    "AB_gen.apply(init_weights_kaiming)\n",
    "BC_gen.apply(init_weights_kaiming)\n",
    "AB_gen = set_device(AB_gen, device, ngpu)\n",
    "BC_gen = set_device(BC_gen, device, ngpu)\n",
    "\n",
    "# setup disciminators for each domain\n",
    "A_disc = DomainDiscs('A', n_channel, n_scale, n_layer, n_disc_filter)\n",
    "B_disc = DomainDiscs('B', n_channel, n_scale, n_layer, n_disc_filter)\n",
    "C_disc = DomainDiscs('C', n_channel, n_scale, n_layer, n_disc_filter)\n",
    "\n",
    "A_disc.apply(init_weights_normal)\n",
    "B_disc.apply(init_weights_normal)\n",
    "C_disc.apply(init_weights_normal)\n",
    "A_disc = set_device(A_disc, device, ngpu)\n",
    "B_disc = set_device(B_disc, device, ngpu)\n",
    "C_disc = set_device(C_disc, device, ngpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimizers\n",
    "AB_g_opt = optim.Adam(AB_gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "BC_g_opt = optim.Adam(BC_gen.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "g_opts = [AB_g_opt, BC_g_opt]\n",
    "\n",
    "A_d_opt = optim.Adam(A_disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "B_d_opt = optim.Adam(B_disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "C_d_opt = optim.Adam(C_disc.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "d_opts = [A_d_opt, B_d_opt, C_d_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup dataloaders\n",
    "A_train = load_train_data(A_train_dir, batch_size)\n",
    "B_train = load_train_data(B_train_dir, batch_size)\n",
    "C_train = load_train_data(C_train_dir, batch_size)\n",
    "\n",
    "A_test = load_test_data(A_test_dir, test_batch_size)\n",
    "B_test = load_test_data(B_test_dir, test_batch_size)\n",
    "C_test = load_test_data(C_test_dir, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.74 GiB total capacity; 9.54 GiB already allocated; 28.69 MiB free; 10.30 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-48d42abf0b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0madv7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macl7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_cycle_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBC_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0madv8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macl8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_cycle_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBC_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0madv9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macl9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_bigcycle_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAB_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBC_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0madv10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macl10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_bigcycle_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBC_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAB_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-89448cec455d>\u001b[0m in \u001b[0;36mgen_bigcycle_grad\u001b[0;34m(input, s, b, t, s_gen, t_gen, s_disc, t_disc, hp)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0ml_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbridge_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ml_mean2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_bridge_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0ml_mean3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecovered_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec_bridge_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# compute regularization losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1117a27f02c6>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, input, source, target, keep_mean, requires_grad)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0ml_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-40f0ba2aba54>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-66ec54909b96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minput\u001b[0m  \u001b[0;31m# apply skip-connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    392\u001b[0m             return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n\u001b[1;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             _pair(0), self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    395\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 15.74 GiB total capacity; 9.54 GiB already allocated; 28.69 MiB free; 10.30 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# train\n",
    "dadv_loss = 0\n",
    "dacl_loss = 0\n",
    "gadv_loss = 0\n",
    "gacl_loss = 0\n",
    "r_loss = 0\n",
    "for epoch in range(n_epochs):\n",
    "    for it, (A_imgs, B_imgs, C_imgs, A_imgs2, B_imgs2, C_imgs2) in enumerate(zip(A_train, B_train, C_train, A_train, B_train, C_train)):\n",
    "        A_imgs = A_imgs[0].to(device)\n",
    "        B_imgs = B_imgs[0].to(device)\n",
    "        C_imgs = C_imgs[0].to(device)\n",
    "        A_imgs2 = A_imgs2[0].to(device)\n",
    "        B_imgs2 = B_imgs2[0].to(device)\n",
    "        C_imgs2 = C_imgs2[0].to(device)\n",
    "\n",
    "        # update discriminators if needed\n",
    "        if it % disc_update_freq == 0:\n",
    "            for d_opt in d_opts:\n",
    "                d_opt.zero_grad()\n",
    "\n",
    "            # extra real examples to mitigate bias in discriminators\n",
    "            adv1, acl1 = disc_ae_grad(A_imgs, AB_gen, A_disc, hp)\n",
    "            adv2, acl2 = disc_ae_grad(B_imgs, AB_gen, B_disc, hp)\n",
    "            adv3, acl3 = disc_ae_grad(B_imgs, BC_gen, B_disc, hp)\n",
    "            adv4, acl4 = disc_ae_grad(C_imgs, BC_gen, C_disc, hp)\n",
    "            adv5, acl5 = disc_ae_grad(A_imgs2, AB_gen, A_disc, hp)\n",
    "            adv6, acl6 = disc_ae_grad(B_imgs2, AB_gen, B_disc, hp)\n",
    "            adv7, acl7 = disc_ae_grad(B_imgs2, BC_gen, B_disc, hp)\n",
    "            adv8, acl8 = disc_ae_grad(C_imgs2, BC_gen, C_disc, hp)\n",
    "\n",
    "            # generate translations\n",
    "            adv9, acl9 = disc_cycle_grad(A_imgs, AB_gen, A_disc, B_disc, hp)\n",
    "            adv10, acl10 = disc_cycle_grad(B_imgs, AB_gen, B_disc, A_disc, hp)\n",
    "            adv11, acl11 = disc_cycle_grad(C_imgs, BC_gen, C_disc, B_disc, hp)\n",
    "            adv12, acl12 = disc_cycle_grad(B_imgs, BC_gen, B_disc, C_disc, hp)\n",
    "            adv13, acl13 = disc_bigcycle_grad(A_imgs, 'A', 'B', 'C', AB_gen, BC_gen, A_disc, C_disc, hp)\n",
    "            adv14, acl14 = disc_bigcycle_grad(C_imgs, 'C', 'B', 'A', BC_gen, AB_gen, C_disc, A_disc, hp)\n",
    "\n",
    "            for d_opt in d_opts:\n",
    "                d_opt.step()\n",
    "\n",
    "            dadv_loss = adv1 + adv2 + adv3 + adv4 + adv5 + adv6 + adv7 + adv8 + adv9 + adv10 + adv11 + adv12 + adv13 + adv14\n",
    "            dacl_loss = acl1 + acl2 + acl3 + acl4 + acl5 + acl6 + acl7 + acl8 + acl9 + acl10 + acl11 + acl12 + acl13 + acl14\n",
    "        \n",
    "        # update generators if needed\n",
    "        if it % gen_update_freq == 0:\n",
    "            for g_opt in g_opts:\n",
    "                g_opt.zero_grad()\n",
    "            \n",
    "            # image reconstructions\n",
    "            r1, acl1 = gen_ae_grad(A_imgs, AB_gen, A_disc, hp)\n",
    "            r2, acl2 = gen_ae_grad(B_imgs, AB_gen, B_disc, hp)\n",
    "            r3, acl3 = gen_ae_grad(B_imgs, BC_gen, B_disc, hp)\n",
    "            r4, acl4 = gen_ae_grad(C_imgs, BC_gen, C_disc, hp)\n",
    "\n",
    "            # generate translations\n",
    "            adv5, acl5 = gen_cycle_grad(A_imgs, AB_gen, A_disc, B_disc, hp)\n",
    "            adv6, acl6 = gen_cycle_grad(B_imgs, AB_gen, B_disc, A_disc, hp)\n",
    "            adv7, acl7 = gen_cycle_grad(C_imgs, BC_gen, C_disc, B_disc, hp)\n",
    "            adv8, acl8 = gen_cycle_grad(B_imgs, BC_gen, B_disc, C_disc, hp)\n",
    "            adv9, acl9 = gen_bigcycle_grad(A_imgs, 'A', 'B', 'C', AB_gen, BC_gen, A_disc, C_disc, hp)\n",
    "            adv10, acl10 = gen_bigcycle_grad(C_imgs, 'C', 'B', 'A', BC_gen, AB_gen, C_disc, A_disc, hp)\n",
    "\n",
    "            for g_opt in g_opts:\n",
    "                g_opt.step()\n",
    "            gadv_loss = adv5 + adv6 + adv7 + adv8 + adv9 + adv10\n",
    "            gacl_loss = acl1 + acl2 + acl3 + acl4 + acl5 + acl6 + acl7 + acl8 + acl9 + acl10\n",
    "            r_loss = r1 + r2 + r3 + r4\n",
    "        \n",
    "        print(f'[{it}], adv: {dadv_loss:.4f}, acl: {dacl_loss:.4f}, gadv: {gadv_loss:.4f}, gacl: {gacl_loss:.4f}, r: {r_loss:.4f}', end='\\n')\n",
    "    # print()  # keep printed stats for the end of an epoch\n",
    "\n",
    "    # checkpoints\n",
    "    save_network(AB_gen, checkpoint_dir, 'AB_latest')\n",
    "    save_network(BC_gen, checkpoint_dir, 'BC_latest')\n",
    "    save_network(A_disc, checkpoint_dir, 'A_disc_latest')\n",
    "    save_network(B_disc, checkpoint_dir, 'B_disc_latest')\n",
    "    save_network(C_disc, checkpoint_dir, 'C_disc_latest')\n",
    "    if (epoch + 1) % checkpoint_freq == 0:\n",
    "        save_network(AB_gen, checkpoint_dir, f'AB_{epoch}')\n",
    "        save_network(BC_gen, checkpoint_dir, f'BC_{epoch}')\n",
    "        save_network(A_disc, checkpoint_dir, f'A_disc_{epoch}')\n",
    "        save_network(B_disc, checkpoint_dir, f'B_disc_{epoch}')\n",
    "        save_network(C_disc, checkpoint_dir, f'C_disc_{epoch}')\n",
    "    \n",
    "    # test images\n",
    "    if (epoch + 1) % test_freq == 0:\n",
    "        s_in = next(iter(A_test))[0].to(device)\n",
    "        b_in = next(iter(B_test))[0].to(device)\n",
    "        t_in = next(iter(C_test))[0].to(device)\n",
    "        test_translate_images(s_in, b_in, t_in, 'A', 'B', 'C', AB_gen, BC_gen, result_dir, str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4d55bb9aa1106442fd435f8f1e04c31548a6171b7907eac5b38f6147c120ad0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
