{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010e136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as tdata\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as tforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4dc8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "img_size = 256\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa91d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str, batch_size=64, num_workers=2):\n",
    "    data_transform = tforms.Compose([\n",
    "        tforms.Resize(img_size),\n",
    "        tforms.CenterCrop(img_size),\n",
    "        tforms.ToTensor(),\n",
    "        tforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = ImageFolder(root=path, transform=data_transform)\n",
    "    return tdata.DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c56379-3722-40fe-9281-5ea1d3e834b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prerec_data = load_data('./prerec_test', batch_size=128)\n",
    "live_data = load_data('./live_test', batch_size=128)\n",
    "real_data = load_data('./real_test', batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826a7cb-5ba3-41c9-8fb0-3266e3b0834e",
   "metadata": {},
   "source": [
    "# Architecture Definition and helper functions\n",
    "\n",
    "The following definitions are copied over from the training code to allow us to load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce17d59-7578-41fe-9c3d-e67807bbde03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_device(model: nn.Module, device, ngpu):\n",
    "    \"\"\"Transfers the models onto the specified device.\n",
    "        \n",
    "    Params:\n",
    "        model -- which model to transfer\n",
    "        device -- which device to use\n",
    "        ngpus -- how many gpus to use, if using cuda device\n",
    "    Returns:\n",
    "        The transferred model\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    if device.type == 'cuda' and ngpu > 1:\n",
    "        model = nn.DataParallel(model, list(range(ngpu)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d8ecb53-60c6-44d3-86cc-36c062c74172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(model: nn.Module, dir_name, name):\n",
    "    \"\"\"Loads a state dictionary from the files in given path.\n",
    "    \n",
    "    Params:\n",
    "        model -- the model to load\n",
    "        dir_name -- directory from which to load the state\n",
    "        name -- name that is prefixed on the files\n",
    "    \"\"\"\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.load_state_dict(torch.load(path.join(dir_name, f'{name}.pth')))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(path.join(dir_name, f'{name}.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70076db-0a7c-470b-b537-b15677a196c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_norm_relu(n_in, n_out, kernel_size, stride, padding=0, padding_mode='reflect', transpose=False, **kwargs):\n",
    "    \"\"\"Standard convolution -> instance norm -> relu block.\n",
    "\n",
    "    Params:\n",
    "        n_in -- number of input channels\n",
    "        n_out -- number of filters/output channels\n",
    "        kernel_size -- passed to Conv2d\n",
    "        stride -- passed to Conv2d\n",
    "        padding -- passed to Conv2d\n",
    "        padding_mode -- passed to Conv2d\n",
    "        transpose -- whether to use a regular or transposed convolution layer\n",
    "        kwargs -- other args passed to Conv2d\n",
    "    Returns:\n",
    "        A list containing a convolution, instance norm, and ReLU activation\n",
    "    \"\"\"\n",
    "    if transpose:\n",
    "        conv = nn.ConvTranspose2d(n_in, n_out, kernel_size, stride, padding, padding_mode=padding_mode, bias=True, **kwargs)\n",
    "    else:\n",
    "        conv = nn.Conv2d(n_in, n_out, kernel_size, stride, padding, padding_mode=padding_mode, bias=True, **kwargs)\n",
    "    return [conv, nn.InstanceNorm2d(n_out), nn.ReLU(True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de9e1681-85d9-467a-853a-73419a9943cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_norm_leakyrelu(n_in, n_out, slope=0.2, **kwargs):\n",
    "    \"\"\"Standard convolution -> instance norm -> leaky relu block.\n",
    "    \n",
    "    Params:\n",
    "        n_in -- number of input channels\n",
    "        n_out -- number of filters/output channels\n",
    "        slope -- slope of the leaky ReLU layer\n",
    "        kwargs -- other args passed to the convolution layer\n",
    "    Returns:\n",
    "        A list containing a convolution, instance norm, and LeakyReLU activation\n",
    "    \"\"\"\n",
    "    conv = nn.Conv2d(n_in, n_out, **kwargs)\n",
    "    return [conv, nn.InstanceNorm2d(n_out), nn.LeakyReLU(slope, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14848026-408d-430b-8306-00fc101725ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Defines a residual block with 2 3x3 conv-norm-relu layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, p=None):\n",
    "        \"\"\"Initialize a residual block.\n",
    "        \n",
    "        Params:\n",
    "            k -- number of input and output channels\n",
    "            p -- dropout rate (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        model = conv_norm_relu(k, k, 3, 1, 1)\n",
    "        model.append(nn.Conv2d(k, k, 3, 1, 1, padding_mode='reflect', bias=True))\n",
    "        model.append(nn.InstanceNorm2d(k))\n",
    "        if p is not None:\n",
    "            model.append(nn.Dropout(p, inplace=True))\n",
    "        self.block = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        residual = self.block(input)\n",
    "        residual += input  # apply skip-connection\n",
    "        return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0499d7b4-f48b-433c-a923-745c9889be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Convolutional-Resnet style encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_head, n_res, in_channel, n_filter):\n",
    "        \"\"\"Initialize an encoder.\n",
    "        \n",
    "        Params:\n",
    "            n_head -- number of downsampling convolution blocks at the head\n",
    "            n_res -- number of residual blocks in the middle\n",
    "            in_channel -- number of channels in the input\n",
    "            n_filter -- number of filters to start with; doubles for each block in the head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # initial convolution\n",
    "        front = conv_norm_relu(in_channel, n_filter, 7, 1, 3)\n",
    "        # downsampling convolution blocks\n",
    "        for _ in range(n_head):\n",
    "            front += conv_norm_relu(n_filter, 2 * n_filter, 4, 2, 1)\n",
    "            n_filter *= 2\n",
    "        # middle residual blocks\n",
    "        front += [ResidualBlock(n_filter) for _ in range(n_res)]\n",
    "        self.model = nn.Sequential(*front)\n",
    "        self.out_channel = n_filter  # record the number of filters before the adjustment\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8d8685c-565c-4bf3-b12b-bad10ed779fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentAE(nn.Module):\n",
    "    \"\"\"Shared latent space VAE. Contains both encoder and decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_res, in_channel, p=None):\n",
    "        \"\"\"Initialize a VAE.\n",
    "\n",
    "        Params:\n",
    "            n_res -- number of residual blocks for both the encoder and decoder\n",
    "            n_channels -- number of channels in the input\n",
    "            p -- dropout probability used (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(*[ResidualBlock(in_channel) for _ in range(n_res)])\n",
    "        self.dec = nn.Sequential(*[ResidualBlock(in_channel, p) for _ in range(n_res)])\n",
    "        self.is_dist = False\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent_mean = self.enc(input)\n",
    "        sample = latent_mean + torch.randn(latent_mean.size(), device=latent_mean.device)\n",
    "        return latent_mean, self.dec(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b111579-f94e-4918-bef8-195a424f422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Convolutional-Resnet style decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, n_tail, n_res, in_channel, out_channel, p=None):\n",
    "        \"\"\"Initialize a decoder.\n",
    "\n",
    "        Params:\n",
    "            n_tail -- number of upsampling convolution blocks at the tail\n",
    "            n_res -- number of residual blocks in the middle\n",
    "            in_channel -- number of channels in the input\n",
    "            out_channel -- desired number of channels in the output\n",
    "            p -- dropout probability used (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # residual blocks in the middle\n",
    "        model = [ResidualBlock(in_channel, p) for _ in range(n_res)]\n",
    "        # upsampling transposed convolution blocks\n",
    "        for _ in range(n_tail):\n",
    "            model += conv_norm_relu(in_channel, in_channel // 2, 4, 2, 1, padding_mode='zeros', transpose=True)\n",
    "            in_channel //= 2\n",
    "        # final convolution (use tanh)\n",
    "        model += [nn.Conv2d(in_channel, out_channel, 7, 1, 3, padding_mode='reflect', bias=True), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78fe3a0-92db-466d-a57e-59409f10bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    \"\"\"Wraps the models necessary to perform translation between two domains.\"\"\"\n",
    "\n",
    "    def __init__(self, d1: str, d2: str, n_channel, n_conv, n_res, n_shared, n_filter, p=None):\n",
    "        \"\"\"Initializes two VAEs with shared inner weights.\n",
    "\n",
    "        Params:\n",
    "            d1 -- name of first domain\n",
    "            d2 -- name of second domain\n",
    "            n_channel -- number of input channels of an image\n",
    "            n_conv -- number of outermost conv/conv-tranpose blocks in the VAE\n",
    "            n_res -- number of residual blocks in the middle layers of the VAE\n",
    "            n_shared -- number of residual blocks that are shared\n",
    "            n_filter -- number of filters to start with in the encoder\n",
    "            p -- dropout probability in the decoders (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        d1_encoder = Encoder(n_conv, n_res, n_channel, n_filter)\n",
    "        d2_encoder = Encoder(n_conv, n_res, n_channel, n_filter)\n",
    "        d1_decoder = Decoder(n_conv, n_res, d1_encoder.out_channel, n_channel, p)\n",
    "        d2_decoder = Decoder(n_conv, n_res, d2_encoder.out_channel, n_channel, p)\n",
    "        self.encoders = nn.ModuleDict({d1: d1_encoder, d2: d2_encoder})\n",
    "        self.decoders = nn.ModuleDict({d1: d1_decoder, d2: d2_decoder})\n",
    "        self.shared = LatentAE(n_shared, d1_encoder.out_channel, p)\n",
    "\n",
    "    def translate(self, input, source: str, target: str, keep_mean=True, requires_grad=True):\n",
    "        \"\"\"Translates a batch of images from the source domain to the target domain.\n",
    "\n",
    "        Params:\n",
    "            input -- input image (batch)\n",
    "            source -- source domain\n",
    "            target -- target domain (of translation)\n",
    "            keep_mean -- whether to also return the latent space mean\n",
    "            requires_grad -- whether to track the computation graph\n",
    "        Returns:\n",
    "            The translated image\n",
    "        \"\"\"\n",
    "        if requires_grad:\n",
    "            l_mean, encoding = self.shared(self.encoders[source](input))\n",
    "            output = self.decoders[target](encoding)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                l_mean, encoding = self.shared(self.encoders[source](input))\n",
    "                output = self.decoders[target](encoding)\n",
    "        if keep_mean:\n",
    "            return l_mean, output\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e698691-16e2-4c36-a6b0-c3e329505fb1",
   "metadata": {},
   "source": [
    "# Metric Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bb2f2a-4bfe-44bf-bd97-b40ce8d24006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid(model: nn.Module, source_data: tdata.DataLoader, target_data: tdata.DataLoader):\n",
    "    \"\"\"Computes the FID for a model.\n",
    "    \n",
    "    Params:\n",
    "        model -- a network translating the source domain to target domain\n",
    "        target_data -- loads data from the target domain ('real' data)\n",
    "        source_data -- loads data from the source domain (is translated into 'fake' data)\n",
    "    Returns:\n",
    "        FID value\n",
    "    \"\"\"\n",
    "    fid = FrechetInceptionDistance()\n",
    "    \n",
    "    # feed fake/translated data in\n",
    "    for source_batch in source_data:\n",
    "        source_batch = source_batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            fake_batch = model(source_batch)\n",
    "            # need to convert image to correct format\n",
    "            fid.update(fake_batch.mul(0.5).add_(0.5).mul(255).add_(0.5).clamp_(0, 255).to('cpu', dtype=torch.uint8), real=False)\n",
    "    \n",
    "    # feed real data in\n",
    "    for target_batch in target_data:\n",
    "        with torch.no_grad():\n",
    "            # need to convert image to correct format\n",
    "            img = target_batch[0].mul(0.5).add_(0.5).mul(255).add_(0.5).clamp_(0, 255).to(dtype=torch.uint8)\n",
    "            fid.update(img, real=True)\n",
    "    \n",
    "    return fid.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9385dfb2-735c-444a-a5a8-d5dbe9d6e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_is(model: nn.Module, source_data: tdata.DataLoader):\n",
    "    \"\"\"Computes the IS for a model.\n",
    "    \n",
    "    Params:\n",
    "        model -- a network translating the source domain to target domain\n",
    "        source_data -- loads data from the source domain (is translated into 'fake' data)\n",
    "    Returns:\n",
    "        IS mean and stddev\n",
    "    \"\"\"\n",
    "    inception = InceptionScore()\n",
    "    \n",
    "    # feed generated images\n",
    "    for source_batch in source_data:\n",
    "        source_batch = source_batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            fake_batch = model(source_batch)\n",
    "            # need to convert image to correct format\n",
    "            inception.update(fake_batch.mul(0.5).add_(0.5).mul(255).add_(0.5).clamp_(0, 255).to('cpu', dtype=torch.uint8))\n",
    "    \n",
    "    is_mean, is_stddev = inception.compute()\n",
    "    return is_mean.item(), is_stddev.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01450f76-c4fb-49a7-a969-5c375912a78c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d69404ff-775d-4815-8dc5-51926b2b57bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# architecture parameters (see DomainModels constructor for details)\n",
    "n_channel = 3\n",
    "n_conv = 3\n",
    "n_res = 3\n",
    "n_shared = 1  # number of shared layers, in both encoder and decoder\n",
    "n_gen_filter = 64\n",
    "n_scale = 1  # number of scales to apply MS-disc at\n",
    "n_layer = 5  # number of layers for the discriminator\n",
    "n_disc_filter = 64\n",
    "p = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de136d6c-7e31-496d-9eea-a2750c4b93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup translators between domains\n",
    "AB_gen = Translator('A', 'B', n_channel, n_conv, n_res, n_shared, n_gen_filter, p)\n",
    "BC_gen = Translator('B', 'C', n_channel, n_conv, n_res, n_shared, n_gen_filter, p)\n",
    "\n",
    "load_network(AB_gen, 'checkpoints', 'AB_20')\n",
    "load_network(BC_gen, 'checkpoints', 'BC_20')\n",
    "AB_gen = set_device(AB_gen, device, 1)\n",
    "BC_gen = set_device(BC_gen, device, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12981de",
   "metadata": {},
   "source": [
    "# Evaluate models between prerecorded pizza and real pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e4d63",
   "metadata": {},
   "source": [
    "### prerec2real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3430db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source2Target(nn.Module):\n",
    "    \"\"\"Wrapper for translating inputs from source to target, via the bridge.\"\"\"\n",
    "    \n",
    "    def __init__(self, s2b: Translator, b2t: Translator, source: str, bridge: str, target: str):\n",
    "        \"\"\"Initialize the Module.\n",
    "        \n",
    "        Params:\n",
    "            s2b -- translator to convert between source and bridge domains\n",
    "            b2t -- translator to convert between bridge and target domains\n",
    "            source -- source domain name\n",
    "            bridge -- bridge domain name\n",
    "            target -- target domain name\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.s2b = s2b\n",
    "        self.b2t = b2t\n",
    "        self.s = source\n",
    "        self.b = bridge\n",
    "        self.t = target\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bridge_img = self.s2b.translate(input, self.s, self.b, keep_mean=False, requires_grad=False)\n",
    "        return self.b2t.translate(bridge_img, self.b, self.t, keep_mean=False, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9b591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prerec2real = Source2Target(AB_gen, BC_gen, 'A', 'B', 'C')\n",
    "real2prerec = Source2Target(BC_gen, AB_gen, 'C', 'B', 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538ce9be-4753-481a-a3df-8664f5024df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/home/jl2362/.local/lib/python3.6/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `FrechetInceptionDistance` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/common/home/jl2362/.local/lib/python3.6/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 158.96975708007812\n",
      "is: 2.973461389541626 ± 0.2130606323480606\n"
     ]
    }
   ],
   "source": [
    "# compute scores for prerecorded -> real\n",
    "fid = compute_fid(prerec2real, prerec_data, real_data)\n",
    "i_score = compute_is(prerec2real, prerec_data)\n",
    "\n",
    "print(f'fid: {fid}')\n",
    "print(f'is: {i_score[0]} \\u00B1 {i_score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "812d2e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 194.5352325439453\n",
      "is: 2.964608669281006 ± 0.1492490917444229\n"
     ]
    }
   ],
   "source": [
    "# compute scores for real -> prerecorded\n",
    "fid = compute_fid(real2prerec, real_data, prerec_data)\n",
    "i_score = compute_is(real2prerec, real_data)\n",
    "\n",
    "print(f'fid: {fid}')\n",
    "print(f'is: {i_score[0]} \\u00B1 {i_score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a11a9",
   "metadata": {},
   "source": [
    "### live2real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0bc73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Source2Target(nn.Module):\n",
    "    \"\"\"Wrapper for translating inputs directly from source to target (no bridge).\"\"\"\n",
    "    \n",
    "    def __init__(self, model: Translator, source: str, target: str):\n",
    "        \"\"\"Initialize the Module.\n",
    "        \n",
    "        Params:\n",
    "            model -- translator to convert between source and target domains\n",
    "            source -- source domain name\n",
    "            bridge -- bridge domain name\n",
    "            target -- target domain name\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.s = source\n",
    "        self.t = target\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.model.translate(input, self.s, self.t, keep_mean=False, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd4fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "live2real = Source2Target(BC_gen, 'B', 'C')\n",
    "real2live = Source2Target(BC_gen, 'C', 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61e80b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 132.95069885253906\n",
      "is: 2.061117172241211 ± 0.07366422563791275\n"
     ]
    }
   ],
   "source": [
    "# compute scores for live -> real\n",
    "fid = compute_fid(live2real, live_data, real_data)\n",
    "i_score = compute_is(live2real, live_data)\n",
    "\n",
    "print(f'fid: {fid}')\n",
    "print(f'is: {i_score[0]} \\u00B1 {i_score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdfe8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid: 89.31855773925781\n",
      "is: 2.04569149017334 ± 0.07454477250576019\n"
     ]
    }
   ],
   "source": [
    "# compute scores for real -> prerecorded\n",
    "fid = compute_fid(real2live, real_data, live_data)\n",
    "i_score = compute_is(real2live, real_data)\n",
    "\n",
    "print(f'fid: {fid}')\n",
    "print(f'is: {i_score[0]} \\u00B1 {i_score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130b8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
